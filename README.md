This is continued work, under the guidance of Professor Xiao, of the work done by Michale Reinisch et al. https://arxiv.org/abs/2408.10995


To be absolutely clear, a lot of this code is not mine. It is the continuation of Michael's repository. With that said I have developed quite a bit, all of which will be explicilty and clearly explained in my written report.


In brief, I have edited, added, or made siginificant changes within the following folders:
  ctp-llm/
      bert_rf : cleaned up methods, increased simulation runs to gather better evaluation metrics, tsne analysis, error analysis
      gpt_fine_tuning: fine tune gpt 4 models, started gpt-4 zero-shot/few-shot for classification
      llama_pretraining: fine tuning medical llama, standard llama, error analysis, few shot prompting, zero shot prompting
